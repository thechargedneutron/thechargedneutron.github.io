---
title: "SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024"
excerpt2: Changan Chen, <u>Kumar Ashutosh</u>, Rohit Girdhar, David Harwath, Kristen Grauman
paper_link: https://changan.io/files/SoundingActions.pdf
project_page: https://vision.cs.utexas.edu/projects/soundingactions/
externalLink: false
year: year2017
image:
  feature: "sounding_actions.png"
---

Please refer to the project page: [https://vision.cs.utexas.edu/projects/soundingactions/](https://vision.cs.utexas.edu/projects/soundingactions/) and the CVPR 2024 paper: [https://changan.io/files/SoundingActions.pdf](https://changan.io/files/SoundingActions.pdf)

### Abstract &nbsp;

We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how subtle and long-tail human actions sound in egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.