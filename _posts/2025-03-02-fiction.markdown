---
title: "FIction: 4D Future Interaction Prediction from Video"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025  <br> <b style='color:red;'>Highlight Presentation</b>"
excerpt2: <u>Kumar Ashutosh</u>, Georgios Pavlakos, Kristen Grauman
paper_link: https://arxiv.org/abs/2412.00932
project_page: https://vision.cs.utexas.edu/projects/FIction/
externalLink: false
year: year2017
presentation: oral
image:
  feature: "teaser_fiction.png"
---

Please refer to the paper: [[https://arxiv.org/abs/2412.00932](https://arxiv.org/abs/2412.00932)].

### Abstract &nbsp;

Anticipating how a person will interact with objects in an environment is essential for activity understanding, but existing methods are limited to the 2D space of video frames-capturing physically ungrounded predictions of 'what' and ignoring the 'where' and 'how'. We introduce 4D future interaction prediction from videos. Given an input video of a human activity, the goal is to predict what objects at what 3D locations the person will interact with in the next time period (e.g., cabinet, fridge), and how they will execute that interaction (e.g., poses for bending, reaching, pulling). We propose a novel model FIction that fuses the past video observation of the person's actions and their environment to predict both the 'where' and 'how' of future interactions. Through comprehensive experiments on a variety of activities and real-world environments in Ego-Exo4D, we show that our proposed approach outperforms prior autoregressive and (lifted) 2D video models substantially, with more than 30% relative gains.
