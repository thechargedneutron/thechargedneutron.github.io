---
title: "FIction: 4D Future Interaction Prediction from Video"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025"
excerpt2: <u>Kumar Ashutosh</u>, Georgios Pavlakos, Kristen Grauman
paper_link: https://arxiv.org/abs/2412.00932
project_page: https://vision.cs.utexas.edu/projects/FIction/
externalLink: false
year: year2017
image:
  feature: "teaser_fiction.png"
---

Please refer to the paper: [https://arxiv.org/abs/2408.00672](https://arxiv.org/abs/2408.00672). Project page will be available soon.

### Abstract &nbsp;

Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback from video of a person doing a physical activity, such as basketball or soccer. Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full-spectrum, actionable coaching -- expert commentary, expert video retrieval, and the first-of-its-kind expert pose generation -- outperforming strong vision-language models on both established metrics and human preference studies.
