---
title: "Learning Object State Changes in Videos: An Open-World Perspective"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "ArXiv 2023"
excerpt2: Zihui Xue, <u>Kumar Ashutosh</u>, Kristen Grauman
paper_link: https://arxiv.org/abs/2312.11782
project_page: https://vision.cs.utexas.edu/projects/VidOSC/
externalLink: false
year: year2017
image:
  feature: "intro_oscc.png"
---

Please refer to the project page: [https://vision.cs.utexas.edu/projects/VidOSC/](https://vision.cs.utexas.edu/projects/VidOSC/) and the paper: [https://arxiv.org/abs/2312.11782](https://arxiv.org/abs/2312.11782)

### Abstract &nbsp;

Object State Changes (OSCs) are pivotal for video understanding. While humans can effortlessly generalize OSC understanding from familiar to unknown objects, current approaches are confined to a closed vocabulary. Addressing this gap, we introduce a novel open-world formulation for the video OSC problem. The goal is to temporally localize the three stages of an OSC---the object's initial state, its transitioning state, and its end state---whether or not the object has been observed during training. Towards this end, we develop VidOSC, a holistic learning approach that: (1) leverages text and vision-language models for supervisory signals to obviate manually labeling OSC training data, and (2) abstracts fine-grained shared state representations from objects to enhance generalization. Furthermore, we present HowToChange, the first open-world benchmark for video OSC localization, which offers an order of magnitude increase in the label space and annotation volume compared to the best existing benchmark. Experimental results demonstrate the efficacy of our approach, in both traditional closed-world and open-world scenarios.