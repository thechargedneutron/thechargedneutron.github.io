---
title: "LLMs can see and hear without any training"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "42nd International Conference on Machine Learning (ICML), July 2025"
excerpt2: <u>Kumar Ashutosh</u>, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar
paper_link: https://arxiv.org/abs/2501.18096
project_page: https://github.com/facebookresearch/MILS/
externalLink: false
year: year2017
image:
  feature: "mils.png"
---

Please refer to the paper: [[https://arxiv.org/abs/2501.18096](https://arxiv.org/abs/2501.18096)].

### Abstract &nbsp;

We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities into your favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.