---
title: "Stitch-a-Recipe: Video Demonstration from Multistep Descriptions"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "ArXiv 2025"
excerpt2: Chi Hsuan Wu<sup>*</sup>, <u>Kumar Ashutosh<sup>*</sup></u>, Kristen Grauman
paper_link: https://arxiv.org/abs/2503.13821
project_page: https://vision.cs.utexas.edu/projects/stitch-a-recipe/
externalLink: false
year: year2017
image:
  feature: "stitch.png"
---

Please refer to the paper: [[https://arxiv.org/abs/2503.13821](https://arxiv.org/abs/2503.13821)].

### Abstract &nbsp;

When obtaining visual illustrations from text descriptions, today's methods take a description with—a single text context caption, or an action description—and retrieve or generate the matching visual context. However, prior work does not permit visual illustration of multistep descriptions, e.g. a cooking recipe composed of multiple steps. Furthermore, simply handling each step description in isolation would result in an incoherent demonstration. We propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video demonstration from a multistep description. The resulting video contains clips, possibly from different sources, that accurately reflect all the step descriptions, while being visually coherent. We formulate a training pipeline that creates large-scale weakly supervised data containing diverse and novel recipes and injects hard negatives that promote both correctness and coherence. Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves state-of-the-art performance, with quantitative gains up to 24% as well as dramatic wins in a human preference study.