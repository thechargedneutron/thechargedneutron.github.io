---
title: "What You Say Is What You Show: Visual Narration Detection in Instructional Videos"
layout: post
<!-- date: 2016-01-23 22:10 -->
tag: vision
image: /assets/images/nus-logo.jpg
headerImage: false
projects: true
hidden: true # don't count this post in blog pagination
subtitle: Includes projects prior to May 2018
description: 
category: publication-ut
<!-- author: kumarashutosh -->
excerpt: "ArXiv preprint, 2023"
excerpt2: <u>Kumar Ashutosh</u>, Rohit Girdhar, Lorenzo Torresani, Kristen Grauman
paper_link: https://arxiv.org/abs/2301.02307
externalLink: false
year: year2017
image:
  feature: "wys2-teaser.png"
---

### Abstract &nbsp;

Narrated "how-to" videos have emerged as a promising data source for a wide range of learning problems, from learning visual representations to training robot policies. However, this data is extremely noisy, as the narrations do not always describe the actions demonstrated in the video. To address this problem we introduce the novel task of visual narration detection, which entails determining whether a narration is visually depicted by the actions in the video. We propose "What You Say is What You Show" (WYS^2), a method that leverages multi-modal cues and pseudo-labeling to learn to detect visual narrations with only weakly labeled data. We further generalize our approach to operate on only audio input, learning properties of the narrator's voice that hint if they are currently doing what they describe. Our model successfully detects visual narrations in in-the-wild videos, outperforming strong baselines, and we demonstrate its impact for state-of-the-art summarization and alignment of instructional video.

Please refer to the ArXiv paper: [https://arxiv.org/abs/2301.02307](https://arxiv.org/abs/2301.02307)